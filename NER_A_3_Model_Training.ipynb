{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/horasan/eng_to_sql_ner/blob/main/NER_A_3_Model_Training.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#!pip install transformers\n",
        "#!pip install datasets\n",
        "#!pip install seqeval"
      ],
      "metadata": {
        "id": "ox-SnAThZbdT"
      },
      "id": "ox-SnAThZbdT",
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "id": "2fa5e283",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2fa5e283",
        "outputId": "921e96af-d9b0-4a06-a57e-977b7f63b1a6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "# read data from google drive\n",
        "drive.mount('/content/drive')\n",
        "FOLDER_PATH = \"NER_for_SQL\"\n",
        "FULL_PATH = \"/content/drive/My Drive/Colab Notebooks/\" + FOLDER_PATH + \"/\""
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "FOLDER_PATH = \"NER_for_SQL\"\n",
        "FULL_PATH = \"/content/drive/My Drive/Colab Notebooks/\" + FOLDER_PATH + \"/\"\n",
        "bio_tagged_dataset_file_name   = \"synthetic_queries_300_bio_tagged.txt\"\n",
        "\n",
        "tag2id_with_cust_file_name = \"tag2id_with_cust.json\"\n",
        "id2tag_with_cust_file_name = \"id2tag_with_cust.json\"\n",
        "\n",
        "trained_model_path = FULL_PATH + \"ner-roberta-with-cust\"\n",
        "trained_tokenizer_path = FULL_PATH + \"ner-roberta-with-cust\""
      ],
      "metadata": {
        "id": "9kyxrquXZyGa"
      },
      "id": "9kyxrquXZyGa",
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "d9e8d920",
      "metadata": {
        "id": "d9e8d920"
      },
      "source": [
        "# utils"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "id": "feaa711b",
      "metadata": {
        "id": "feaa711b"
      },
      "outputs": [],
      "source": [
        "def load_bio_tagged_data(filepath):\n",
        "    \"\"\"\n",
        "    Reads a BIO-tagged file and returns a list of (tokens, tags) tuples per sentence.\n",
        "\n",
        "    Args:\n",
        "        filepath (str): Path to the BIO-tagged data file.\n",
        "\n",
        "    Returns:\n",
        "        List[Tuple[List[str], List[str]]]: List of sentences with tokens and tags.\n",
        "    \"\"\"\n",
        "    sentences = []\n",
        "    tokens = []\n",
        "    tags = []\n",
        "\n",
        "    with open(filepath, 'r', encoding='utf-8') as f:\n",
        "        for line in f:\n",
        "            line = line.strip()\n",
        "\n",
        "            # Empty line = end of sentence\n",
        "            if not line:\n",
        "                if tokens:\n",
        "                    sentences.append((tokens, tags))\n",
        "                    tokens = []\n",
        "                    tags = []\n",
        "            else:\n",
        "                # Split line into token and tag\n",
        "                parts = line.split()\n",
        "                if len(parts) == 2:\n",
        "                    token, tag = parts\n",
        "                    tokens.append(token)\n",
        "                    tags.append(tag)\n",
        "                else:\n",
        "                    raise ValueError(f\"Malformed line: {line}\")\n",
        "\n",
        "        # Catch any remaining sentence at EOF\n",
        "        if tokens:\n",
        "            sentences.append((tokens, tags))\n",
        "\n",
        "    return sentences\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "id": "b5d01e2b",
      "metadata": {
        "id": "b5d01e2b"
      },
      "outputs": [],
      "source": [
        "import random\n",
        "\n",
        "def split_train_test(data, test_size=0.2, seed=42):\n",
        "    \"\"\"\n",
        "    Splits BIO-tagged (tokens, tags) data into train and test sets.\n",
        "\n",
        "    Args:\n",
        "        data (list of tuples): Each item is a (tokens, tags) pair.\n",
        "        test_size (float): Proportion of data to include in the test set.\n",
        "        seed (int): Random seed for reproducibility.\n",
        "\n",
        "    Returns:\n",
        "        Tuple: (train_data, test_data)\n",
        "    \"\"\"\n",
        "    random.seed(seed)\n",
        "    random.shuffle(data)\n",
        "\n",
        "    split_index = int(len(data) * (1 - test_size))\n",
        "    train_data = data[:split_index]\n",
        "    test_data = data[split_index:]\n",
        "\n",
        "    return train_data, test_data\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "id": "3b3cf808",
      "metadata": {
        "id": "3b3cf808"
      },
      "outputs": [],
      "source": [
        "def prepare_dataset_for_tokenization(data):\n",
        "    \"\"\"\n",
        "    Converts a list of (tokens, tags) tuples to a dict of lists.\n",
        "    \"\"\"\n",
        "    return [{\"tokens\": tokens, \"tags\": tags} for tokens, tags in data]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "id": "dd2cf074",
      "metadata": {
        "id": "dd2cf074"
      },
      "outputs": [],
      "source": [
        "def build_tag2id_from_data(data):\n",
        "    \"\"\"\n",
        "    Builds a tag2id and id2tag mapping from a dataset with 'tags' field.\n",
        "\n",
        "    Args:\n",
        "        data (list): A list of dicts, each with a 'tags' key.\n",
        "\n",
        "    Returns:\n",
        "        tag2id (dict): Mapping from tag to unique ID.\n",
        "        id2tag (dict): Reverse mapping from ID to tag.\n",
        "    \"\"\"\n",
        "    unique_tags = set(tag for sample in data for tag in sample[\"tags\"])\n",
        "    sorted_tags = sorted(unique_tags)  # Optional: sort for consistency\n",
        "    tag2id = {tag: idx for idx, tag in enumerate(sorted_tags)}\n",
        "    id2tag = {idx: tag for tag, idx in tag2id.items()}\n",
        "    return tag2id, id2tag"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "id": "ccddd9b2",
      "metadata": {
        "id": "ccddd9b2"
      },
      "outputs": [],
      "source": [
        "from transformers import RobertaTokenizerFast\n",
        "\n",
        "def tokenize_and_align_labels(examples, tokenizer, tag2id, max_length=128):\n",
        "    tokenized_inputs = []\n",
        "    labels = []\n",
        "\n",
        "    for item in examples:\n",
        "        tokens = item['tokens']\n",
        "        tags = item['tags']\n",
        "\n",
        "        # Use the tokenizer to get subwords and mapping\n",
        "        tokenized = tokenizer(tokens,\n",
        "                              is_split_into_words=True,\n",
        "                              truncation=True,\n",
        "                              padding='max_length',\n",
        "                              max_length=max_length,\n",
        "                              return_offsets_mapping=True)\n",
        "\n",
        "        word_ids = tokenized.word_ids()\n",
        "        label_ids = []\n",
        "        previous_word_idx = None\n",
        "\n",
        "        for word_idx in word_ids:\n",
        "            if word_idx is None:\n",
        "                label_ids.append(-100)  # Ignored in loss computation\n",
        "            elif word_idx != previous_word_idx:\n",
        "                label_ids.append(tag2id[tags[word_idx]])\n",
        "            else:\n",
        "                # For subwords, use I- tag if it was B-, or repeat O\n",
        "                prev_label = tags[word_idx]\n",
        "                if prev_label.startswith(\"B-\"):\n",
        "                    i_label = \"I-\" + prev_label[2:]\n",
        "                    if i_label in tag2id:\n",
        "                        label_ids.append(tag2id[i_label])\n",
        "                    else:\n",
        "                        label_ids.append(tag2id[prev_label])\n",
        "                else:\n",
        "                    label_ids.append(tag2id[prev_label])\n",
        "\n",
        "            previous_word_idx = word_idx\n",
        "\n",
        "        tokenized[\"labels\"] = label_ids\n",
        "        tokenized_inputs.append(tokenized)\n",
        "\n",
        "    return tokenized_inputs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "id": "00591aa1",
      "metadata": {
        "id": "00591aa1"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "from sklearn.metrics import classification_report\n",
        "\n",
        "def compute_metrics(p):\n",
        "    predictions, labels = p\n",
        "    predictions = np.argmax(predictions, axis=2)\n",
        "\n",
        "    true_predictions = [\n",
        "        [id2tag[pred] for (pred, label) in zip(prediction, label) if label != -100]\n",
        "        for prediction, label in zip(predictions, labels)\n",
        "    ]\n",
        "    true_labels = [\n",
        "        [id2tag[label] for (pred, label) in zip(prediction, label) if label != -100]\n",
        "        for prediction, label in zip(predictions, labels)\n",
        "    ]\n",
        "\n",
        "    report = classification_report(\n",
        "        [item for sublist in true_labels for item in sublist],\n",
        "        [item for sublist in true_predictions for item in sublist],\n",
        "        zero_division=0,\n",
        "        output_dict=True\n",
        "    )\n",
        "    return {\n",
        "        \"precision\": report[\"weighted avg\"][\"precision\"],\n",
        "        \"recall\": report[\"weighted avg\"][\"recall\"],\n",
        "        \"f1\": report[\"weighted avg\"][\"f1-score\"],\n",
        "    }\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "id": "7d567601",
      "metadata": {
        "id": "7d567601"
      },
      "outputs": [],
      "source": [
        "from transformers import RobertaForTokenClassification, Trainer, TrainingArguments\n",
        "from seqeval.metrics import classification_report, accuracy_score, f1_score\n",
        "import numpy as np\n",
        "\n",
        "def compute_metrics(p):\n",
        "    predictions, labels = p\n",
        "    predictions = np.argmax(predictions, axis=2)\n",
        "\n",
        "    true_labels = [\n",
        "        [id2tag[label] for label in label_seq if label != -100]\n",
        "        for label_seq in labels\n",
        "    ]\n",
        "    true_preds = [\n",
        "        [id2tag[pred] for pred, label in zip(pred_seq, label_seq) if label != -100]\n",
        "        for pred_seq, label_seq in zip(predictions, labels)\n",
        "    ]\n",
        "\n",
        "    return {\n",
        "        \"accuracy\": accuracy_score(true_labels, true_preds),\n",
        "        \"f1\": f1_score(true_labels, true_preds),\n",
        "        \"report\": classification_report(true_labels, true_preds, digits=4),\n",
        "    }"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6ae96eda",
      "metadata": {
        "id": "6ae96eda"
      },
      "source": [
        "# Prepare for traning"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "id": "14a85cd4",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "14a85cd4",
        "outputId": "3702ee2a-c1bf-4cb6-8880-daaff3dc9d2b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(['Fetch', 'all', 'FX', 'trades', 'with', 'the', 'date', 'today'], ['O', 'O', 'B-DEAL_TYPE', 'O', 'O', 'O', 'O', 'B-VALUE_DATE'])\n"
          ]
        }
      ],
      "source": [
        "#bio_tagged_dataset_file_name = \"synthetic_queries_300_bio_tagged.txt\"\n",
        "bio_tagged_dataset = load_bio_tagged_data(FULL_PATH + bio_tagged_dataset_file_name)\n",
        "\n",
        "# Inspect the first sentence\n",
        "print(bio_tagged_dataset[0])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "id": "7ca3402b",
      "metadata": {
        "id": "7ca3402b"
      },
      "outputs": [],
      "source": [
        "processed_bio_data_sen_bio_data = prepare_dataset_for_tokenization(bio_tagged_dataset)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "id": "b53f3e6d",
      "metadata": {
        "id": "b53f3e6d"
      },
      "outputs": [],
      "source": [
        "train_data, test_data = split_train_test(processed_bio_data_sen_bio_data, test_size=0.005)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "id": "abce9728",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "abce9728",
        "outputId": "aa8ef4f1-3bf8-4df4-bc26-d78bbc1a8946"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training examples: 2985\n",
            "Testing examples: 15\n"
          ]
        }
      ],
      "source": [
        "print(f\"Training examples: {len(train_data)}\")\n",
        "print(f\"Testing examples: {len(test_data)}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "id": "49466ac0",
      "metadata": {
        "id": "49466ac0"
      },
      "outputs": [],
      "source": [
        "tag2id, id2tag = build_tag2id_from_data(processed_bio_data_sen_bio_data)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "id": "01c3b5bf",
      "metadata": {
        "id": "01c3b5bf"
      },
      "outputs": [],
      "source": [
        "import json\n",
        "\n",
        "#tag2id_with_cust_file_name = \"tag2id_with_cust.json\"\n",
        "#id2tag_with_cust_file_name = \"id2tag_with_cust.json\"\n",
        "\n",
        "# Save mappings\n",
        "with open(FULL_PATH + tag2id_with_cust_file_name, \"w\") as f:\n",
        "    json.dump(tag2id, f)\n",
        "\n",
        "id2tag_to_save = {int(k): v for k, v in id2tag.items()}\n",
        "with open(FULL_PATH + id2tag_with_cust_file_name, \"w\") as f:\n",
        "    json.dump(id2tag_to_save, f)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e110f24b",
      "metadata": {
        "id": "e110f24b"
      },
      "outputs": [],
      "source": [
        "tokenizer = RobertaTokenizerFast.from_pretrained(\"roberta-base\", add_prefix_space=True)\n",
        "train_tokenized = tokenize_and_align_labels(train_data, tokenizer, tag2id)\n",
        "test_tokenized = tokenize_and_align_labels(test_data, tokenizer, tag2id)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "bac59964",
      "metadata": {
        "id": "bac59964"
      },
      "source": [
        "## Model development"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "id": "dbd7c95b",
      "metadata": {
        "id": "dbd7c95b"
      },
      "outputs": [],
      "source": [
        "from datasets import Dataset\n",
        "\n",
        "train_dataset = Dataset.from_list(train_tokenized)\n",
        "test_dataset = Dataset.from_list(test_tokenized)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "id": "d5f7dbab",
      "metadata": {
        "id": "d5f7dbab"
      },
      "outputs": [],
      "source": [
        "from transformers import DataCollatorForTokenClassification\n",
        "\n",
        "data_collator = DataCollatorForTokenClassification(tokenizer)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cded9a8d",
      "metadata": {
        "id": "cded9a8d"
      },
      "outputs": [],
      "source": [
        "from transformers import RobertaForTokenClassification\n",
        "\n",
        "num_labels = len(tag2id)\n",
        "model = RobertaForTokenClassification.from_pretrained(\"roberta-base\", num_labels=num_labels)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "id": "7d2c26a6",
      "metadata": {
        "id": "7d2c26a6"
      },
      "outputs": [],
      "source": [
        "from transformers import TrainingArguments\n",
        "\n",
        "training_args = TrainingArguments(\n",
        "    output_dir=\"./ner-roberta-with-cust\",\n",
        "    #evaluation_strategy=\"epoch\",\n",
        "    eval_strategy = \"epoch\",\n",
        "    save_strategy=\"epoch\",\n",
        "    logging_dir=\"./logs-ner-roberta-with-cust\",\n",
        "    learning_rate=2e-5,\n",
        "    per_device_train_batch_size=16,\n",
        "    per_device_eval_batch_size=16,\n",
        "    num_train_epochs=5,\n",
        "    weight_decay=0.01,\n",
        "    report_to=\"none\"\n",
        ")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "id": "5fbc028d",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5fbc028d",
        "outputId": "6fb91627-568d-4112-b22e-d48bf313b1ab"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-32-3811835426>:3: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
            "  trainer = Trainer(\n"
          ]
        }
      ],
      "source": [
        "from transformers import Trainer\n",
        "\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=train_dataset,\n",
        "    eval_dataset=test_dataset,\n",
        "    tokenizer=tokenizer,\n",
        "    data_collator=data_collator,\n",
        "    compute_metrics=compute_metrics,\n",
        ")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "09f07370",
      "metadata": {
        "id": "09f07370"
      },
      "outputs": [],
      "source": [
        "trainer.train()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "id": "cfaffa6f",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cfaffa6f",
        "outputId": "53930022-cb7c-4e5c-ac92-0cac29b2b6a0"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "('/content/drive/My Drive/Colab Notebooks/NER_for_SQL/ner-roberta-with-cust/tokenizer_config.json',\n",
              " '/content/drive/My Drive/Colab Notebooks/NER_for_SQL/ner-roberta-with-cust/special_tokens_map.json',\n",
              " '/content/drive/My Drive/Colab Notebooks/NER_for_SQL/ner-roberta-with-cust/vocab.json',\n",
              " '/content/drive/My Drive/Colab Notebooks/NER_for_SQL/ner-roberta-with-cust/merges.txt',\n",
              " '/content/drive/My Drive/Colab Notebooks/NER_for_SQL/ner-roberta-with-cust/added_tokens.json',\n",
              " '/content/drive/My Drive/Colab Notebooks/NER_for_SQL/ner-roberta-with-cust/tokenizer.json')"
            ]
          },
          "metadata": {},
          "execution_count": 38
        }
      ],
      "source": [
        "# Save the fine-tuned model\n",
        "\n",
        "trainer.save_model(trained_model_path)\n",
        "\n",
        "# Save the tokenizer\n",
        "\n",
        "tokenizer.save_pretrained(trained_tokenizer_path)"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.2"
    },
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 5
}